---
title: "[ML BASIC REVIEW 1]"
categories: 
  - ML
last_modified_at: 2021-01-01 12:00:00
comments: true
use_math: true # MathJax On
---

#### ML_BASIC_REVIEW 1

#### Grdient Descent

1. 직관적인 이해
- 길을 모르더라도 산 정상으로 가는 방법은 간단
- 현재 위치에서 가장 경사가 가파른 방향으로 산을 오르다 보면 언제가는 산 정상에 다다른다.
- 반대로 깊은 골짜기를 찾고 싶은 경우에는 가장 가파른 내리막 방향으로 산을 내려가다 보면 언제가는 산 골짜기에 다다른다. 
- 즉, 이와 같이 특정 함수의 극대점을 찾기 위해 현재 위치에서 gradient 방향으로 이동해가는 방법을 gradient ascent 방법, 극소점을 찾기 위해 gradient 반대 방향으로 이동해가는 방법을 gradient descent 방법이라 부른다. 

2. Gradient란
- 어떤 다변수 함수 f(x1,x2,...,xn)이 있을 때, 함수 f의 gradient는 아래와 같이 정의된다. <br>
  ![gradient](https://user-images.githubusercontent.com/62474292/107191700-e866f200-6a2f-11eb-9e12-c4267701a816.JPG)
- 즉, gradient는 각 변수로의 일차 편미분 값으로 구성되는 벡터를 의미한다.
- 이 gradient 벡터는 함수 f의 값이 가장 가파르게 증가하는 방향을 나타낸다. (벡터의 크기는 그 증가의 가파른 정도, 기울기를 나타냄)
- 반대로, gradient 벡터에 음수를 취하게 되면 이는 함수 f값이 가장 가파르게 감소하는 방향을 나타낸다.
- 이러한 gradient 특성은 함수의 극점을 찾는 용도로 활용될 수 있다.

3. Gradient descent 방법
- 최적화 알고리즘 중 하나로서 gradient의 특성을 활용하여 특정 cost function의 값을 최소화시키기 위한 parameter값을 점진적으로 찾아내는 방법 <br>
  ![gradient_descent](https://user-images.githubusercontent.com/62474292/107192174-a1c5c780-6a30-11eb-9ab2-e07456c7826c.JPG)
- 즉, 어떤 초기값 벡터 x0부터 시작하여 위 식에 따라 gradient 반대 방향으로 x를 조금씩 이동시키면 함수 f(x)가 극소가 되는 x를 찾을 수 있다
- 이 때, lambda는 알고리즘의 수렴속도를 조절하는 paramter로 주로 learning rate이라 일컫어진다.


4. Gradient descent 방법의 문제점
- learning rate이 너무 작으면 학습시간이 오래걸리고 local minimum에 빠질 수 있다.
- learning rate이 너무 크면 알고리즘이 수렴하지 못하고 발산할 수 있다.
- 정답에 가까워질수록 gradient 벡터의 크기가 0에 수렴하기 때문에 수렴속도가 매우 느려진다. 이 경우 learning rate을 크게 해서 수렴속도를 조절하려고 하면 오히려 알고리즘이 발산할 수 있다. (따라서 실제로 사용할때는 구간별로 다른 learning rate를 적용해가면서 수렴속도를 조절한다.)

#### Batch Gradient Descent (BGD)

1. 배경지식 
- 딥러닝에서 Batch란 일괄적으로 처리되는 집단, 즉 한번에 처리되는 데이터의 집합을 의마한다. (iteration 1회당 사용되는 train dataset의 집합)
- 여기서 iteration은 정해진 batch size를 사용하여 학습(forward-backward)를 반복하는 횟수를 의미한다.
- Batch size별로 여러 iteration을 거쳐 전체 train set이 모두 1번 학습된 경우를 1 epoch이라고 한다.

2. Batch Gradient Descent란
- Batch를 전체 Train set으로 사용하는 경우 Batch Gradient Descent라고 한다.
- 전체 Train set에 대한 error를 계산한 후에 gradient를 한번만 계산하여 모델의 parameter를 update시킨다.
- 여기서 혼동되면 안되는 부분이 gradient descent 알고리즘 자체는 loss function을 입력 데이터 x가 아닌 weight paramter w에 대해서 편미분하는 것이므로, 입력 데이터 x의 개수와는 상관이 없다.

3. BGD의 장점
- 전체 데이터에 대해 weight update가 한번에 이루어지기 때문에 SGD보다 update 횟수가 적고 이는 전체 계산 횟수의 감소로 이어진다.
- 전체 데이터에 대해 error값을 기반으로 gradient를 계산하기 때문에 optimal minimum 값으로의 수렴이 안정적으로 진행된다.
- GPU 병렬 처리에 유리하다

4. BGD의 단점
- 1 step에 모든 train dataset을 사용하므로 학습이 오래 걸린다.
- 전체 학습 데이터에 대한 error를 모델의 update가 이루어지기 전까지 기억해야 하므로 더 많은 메모리가 필요하다.
- local optimal 상태가 되면 빠져나오기 힘들다.


#### Stochastic Gradient Descent (SGD)

1. Stochastic Gradient Descent란
- Mini-batch 사이즈가 1인 경우를 Stochastic Gradient Descent라고 한다.
- Stochastic(확률적)이라는 용어는 각 배치를 포함하는 하나의 예가 무작위로 선택된다는 것을 의미한다.
- SGD는 입력 데이터 1개만을 사용하기 때문에 1개의 데이터를 '벡터'로 표현하여 특정 레이어 층에 입력하는 것으로 이해할 수 있다.

2. SGD의 장점
- local minimum에 빠질 리스크가 적다
- 1 step에 걸리는 시간이 짧기 때문에 수렴속도가 상대적으로 빠르다

3. SGD의 단점
- global minimum를 찾지 못 할 가능성이 있다.
- 데이터를 1개씩 처리하기 때문에 GPU의 성능을 전부 활용할 수 없다.


#### Mini-Batch Gradient Descent


#### Momentum



#### RMSprop



#### Adam



![optimizer](https://user-images.githubusercontent.com/62474292/107313562-27e81980-6ad6-11eb-9aad-b5e1e614ad07.JPG)


#### Loss Surface

1. 배경 지식
- Loss Surface를 알기 위해서 공부해야 되는 개념은 Loss function이다. (Cost function이라고도 일컫어진다.)
- Loss function은 모델이 내놓은 예측값과 실제 정답이 얼마나 다른지를 나타내는 지표로서 정의된다. (모델 학습의 목표는 loss값을 최소화하는 W,B를 찾는 것이다)
- Loss function은 대표적으로 MSE(Mean Squared Error)와 CEE(Cross Entropy Error)가 자주 사용되며, 그 외 다양한 loss function이 존재한다. 
  ![loss](https://user-images.githubusercontent.com/62474292/107312680-4baa6000-6ad4-11eb-89f4-dc65630a2ff8.png)

2. Loss Surface란
- 딥러닝에서 모델은 Loss function의 내놓은 값(Loss)을 줄이는 방향으로 학습이 진행된다.
- 이 과정에서 parameter가 변함에 따라서 loss값이 변하게 되는데, parameter 개수가 2개이하가 아닌이상 사람이 알아볼 수 있는 형태로 loss 그래프를 그리기가 어렵다.
- 아래 예시는 parameter가 2개일때 loss값(z축)이 어떻게 변하는지를 나타낸 그림이고 loss값들로 이루어진 surface를 loss surface라고 한다.
  ![loss_surface](https://user-images.githubusercontent.com/62474292/107312859-bd82a980-6ad4-11eb-82ec-7bff3b55d24e.JPG)


#### Sigmoid Function

1. 배경지식
- 딥러닝 네트워크에서는 노드에 들어오는 값들에 대해 곧바로 다음 레이어로 전달하지 않고 주로 비선형 함수를 통과시킨 후 전달하는데, 이 때 사용하는 함수를 activation function이라 한다.
- 비선형 함수를 사용하는 이유는 선형함수를 사용할 경우 층을 깊게 하는 의미가 줄어들기 때문이다 (선형함수 여러번 통과시켜도 결국 선형함수 형태!)

2. Sigmoid 함수란
- activation function으로서 사용되는 함수로서 Logisitc function이라 불리기도 한다. <br>
  ![sigmoid](https://user-images.githubusercontent.com/62474292/107297653-23f8cf00-6ab7-11eb-9228-b773c8075633.JPG)
  ![sig_graph](https://user-images.githubusercontent.com/62474292/107297656-2529fc00-6ab7-11eb-94ad-f5448d021c68.JPG)
- Sigmoid 함수는 함수값이 (0,1)로 제한되며 x가 매운 큰 값을 가지면 함수값은 1에 수렴하고, x가 매우 작은 값을 가지면 함수값은 0에 수렴한다. (x가 0일때는 함수값이 1/2이다)

3. Sigmoid 함수의 단점
- 딥러닝 초기 시절에는 sigmoid가 많이 사용되었던 반면, 최근에는 여러 단점들 때문에 잘 사용하지 않는다.
- 가장 큰 문제점으로서 gradient vanishing 현상이 발생하는데 sigmoid를 미분한 그래프를 보면 x=0 에서 최대값 1/4를 가지고 x값이 일정부분을 넘어가면 함수값이 0에 수렴하게 된다.
- 이는 이후 gradient backpropagation시 이전의 gradient값과 local gradient를 곱해서 error를 전파하는 backpropagation의 특성에 의해 뉴런에 흐르는 gradient값이 사라질 가능성이 크다
- 함수값 중심이 0이 아니기 때문에 학습이 느려질 수 있다. 
- Sigmoid 함수의 출력값은 (0,1) 사이의 양수로 정해지고 이는 다음 레이어의 입력값이 된다. 즉, 다음 레이어의 입력값이 모두 양수가 되는데 이때 발생하는 문제는 아래와 같다. <br>
  ![equ](https://user-images.githubusercontent.com/62474292/107299640-36750780-6abb-11eb-8e2e-8af8b7f4d080.png)
- 즉, parameter의 gradient값은 입력값에 의해 영향을 받게 되며, 입력값이 모두 양수인 경우 모든 parameter의 부호가 같게 된다. 
- 이렇게 되면 gradient descent를 진행할 대 정확한 방향으로 가지 못하고 지그재그로 수렴하는 문제가 발생하여 학습이 느려지게 된다. 

#### Softmax Function

1. Softmax 함수란
- Softmax 함수는 activation function 중 하나로서, 주로 classification 문제를 해결할 때 마지막 레이어의 activation function으로 사용된다. (이로 인해 마지막 레이어의 개수는 클래스의 개수와 동일하게 되고, softmax 함수를 거쳐 나온 출력값 역시 클래스 개수만큼의 차원을 가지게 된다)
- Softmax 함수는 모든 입력값들을 0과 1 사이로 정규화해주는 역할을 하는데 이를 통해 나온 출력값들의 합은 항상 1이 되어 출력값들을 각각의 클래스에 대한 확률값으로 해석할 수 있다.
- class 개수가 3인 classification 문제를 푼다고 가정했을 때, softmax함수가 입력으로 받는 값은 3차원 벡터([z1,z2,z3])이며 출력값은 아래와 같다.<br>
  ![softmax](https://user-images.githubusercontent.com/62474292/107301930-cb79ff80-6abf-11eb-9f1b-d853d2e05ad4.JPG)

2. Softmax vs. Hardmax
- Softmax와 같이 등장하는 개념으로 Hardmax라는 용어가 있는데, 이는 말 그대로 가장 큰 값만을 1로 나머지 값들을 모두 0으로 출력하는 함수이다.

3. 그림을 통한 Softmax 함수의 이해 <br>
  ![그림1](https://user-images.githubusercontent.com/62474292/107302126-31668700-6ac0-11eb-8612-30e1b76ef711.JPG)
  ![그림2](https://user-images.githubusercontent.com/62474292/107302127-3297b400-6ac0-11eb-86ec-d28ff221e4b9.JPG)
  ![그림3](https://user-images.githubusercontent.com/62474292/107302130-33304a80-6ac0-11eb-9c31-c1f10e3ad981.JPG)


### Train set, Validation set, Test set

1. 데이터셋에 대한 이해
- 딥러닝에서 모델을 학습하고 평가하기 위해서는 dataset이 필요하다.
- 이 때 사용하는 dataset을 특성에 맞게 Train set, Validation set, Test set 3가지로 분류한다.

2. Train set
- 모델을 학습하기 위한 dataset이다.
- 어느 상황에서든 꼭 명심해야 하는 사실은 "모델을 학습하는데에는 오직 Train set만 이용한다" 이다.
- Train set을 이용해서 모델 구조에 변화를 주거나 epoch 등 다양한 hyperparameter를 바꿔가면서 다양한 실험을 진행한다.

3. Validation set
- 학습이 완료된 모델을 검증하기 위한 dataset이다.
- 헷갈리면 안되는 사실 중 하나가 validation set은 직접적으로 학습을 시키지는 않지만 '학습'에 관여는 한다.
- Train set처럼 data 자체를 학습시키지는 않지만 hyperparamter튜닝(어떤 parameter로 모델을 설계했을 때 가장 좋은 성능을 보이는지 테스트)과정에서 validation set이 사용된다.
- 딥러닝 모델의 목표는 결국 unseen data에 대해서 좋은 성능을 보이는 것이기에 validation set(unseen data)를 기반으로 가장 좋은 performance를 보이는 hyperparamter를 채택해 모델을 최종 설계한다.
- 즉, validation set이 중요한 이유는 train data에 overfitting되는 현상을 막아줌과 동시에 unseen data에 대해 좋은 성능을 보이는 모델을 설계하기 위함이다.

4. Test set
- 학습과 검증이 완료된 모델의 성능을 평가하기 위한 dataset이다.
- 학습에 사용되지도 않고, validation set과는 다르게 학습에 관여하지도 않는다.
- 순전히 unseen data로서 모델의 최종 성능을 평가하기 위해 사용된다.
