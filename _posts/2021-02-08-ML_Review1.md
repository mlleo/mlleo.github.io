---
title: "[ML BASIC REVIEW 1]"
categories: 
  - ML
last_modified_at: 2021-01-01 12:00:00
comments: true
use_math: true # MathJax On
---

#### ML_BASIC_REVIEW 1

#### Grdient Descent

1. 직관적인 이해
- 길을 모르더라도 산 정상으로 가는 방법은 간단
- 현재 위치에서 가장 경사가 가파른 방향으로 산을 오르다 보면 언제가는 산 정상에 다다른다.
- 반대로 깊은 골짜기를 찾고 싶은 경우에는 가장 가파른 내리막 방향으로 산을 내려가다 보면 언제가는 산 골짜기에 다다른다. 
- 즉, 이와 같이 특정 함수의 극대점을 찾기 위해 현재 위치에서 gradient 방향으로 이동해가는 방법을 gradient ascent 방법, 극소점을 찾기 위해 gradient 반대 방향으로 이동해가는 방법을 gradient descent 방법이라 부른다. 

2. Gradient란
- 어떤 다변수 함수 f(x1,x2,...,xn)이 있을 때, 함수 f의 gradient는 아래와 같이 정의된다. <br>
  ![gradient](https://user-images.githubusercontent.com/62474292/107191700-e866f200-6a2f-11eb-9e12-c4267701a816.JPG)
- 즉, gradient는 각 변수로의 일차 편미분 값으로 구성되는 벡터를 의미한다.
- 이 gradient 벡터는 함수 f의 값이 가장 가파르게 증가하는 방향을 나타낸다. (벡터의 크기는 그 증가의 가파른 정도, 기울기를 나타냄)
- 반대로, gradient 벡터에 음수를 취하게 되면 이는 함수 f값이 가장 가파르게 감소하는 방향을 나타낸다.
- 이러한 gradient 특성은 함수의 극점을 찾는 용도로 활용될 수 있다.

3. Gradient descent 방법
- 최적화 알고리즘 중 하나로서 gradient의 특성을 활용하여 특정 cost function의 값을 최소화시키기 위한 parameter값을 점진적으로 찾아내는 방법 <br>
  ![gradient_descent](https://user-images.githubusercontent.com/62474292/107192174-a1c5c780-6a30-11eb-9ab2-e07456c7826c.JPG)
- 즉, 어떤 초기값 벡터 x0부터 시작하여 위 식에 따라 gradient 반대 방향으로 x를 조금씩 이동시키면 함수 f(x)가 극소가 되는 x를 찾을 수 있다
- 이 때, lambda는 알고리즘의 수렴속도를 조절하는 paramter로 주로 learning rate이라 일컫어진다.

4. Gradient descent 방법의 문제점
- local minimum에 빠질 수 있다.
- 정답에 가까워질수록 gradient 벡터의 크기가 0에 수렴하기 때문에 수렴속도가 매우 느려진다. 이 경우 learning rate을 크게 해서 수렴속도를 조절하려고 하면 오히려 알고리즘이 발산할 수 있다. (따라서 실제로 사용할때는 구간별로 다른 learning rate를 적용해가면서 수렴속도를 조절한다.)


#### Sigmoid Function

1. 배경지식
- 딥러닝 네트워크에서는 노드에 들어오는 값들에 대해 곧바로 다음 레이어로 전달하지 않고 주로 비선형 함수를 통과시킨 후 전달하는데, 이 때 사용하는 함수를 activation function이라 한다.
- 비선형 함수를 사용하는 이유는 선형함수를 사용할 경우 층을 깊게 하는 의미가 줄어들기 때문이다 (선형함수 여러번 통과시켜도 결국 선형함수 형태!)

2. Sigmoid 함수란
- activation function으로서 사용되는 함수로서 Logisitc function이라 불리기도 한다. <br>
  ![sigmoid](https://user-images.githubusercontent.com/62474292/107297653-23f8cf00-6ab7-11eb-9228-b773c8075633.JPG)
  ![sig_graph](https://user-images.githubusercontent.com/62474292/107297656-2529fc00-6ab7-11eb-94ad-f5448d021c68.JPG)
- Sigmoid 함수는 함수값이 (0,1)로 제한되며 x가 매운 큰 값을 가지면 함수값은 1에 수렴하고, x가 매우 작은 값을 가지면 함수값은 0에 수렴한다. (x가 0일때는 함수값이 1/2이다)



#### Softmax Function



### Train set, Validation set, Test set

1. 데이터셋에 대한 이해
- 딥러닝에서 모델을 학습하고 평가하기 위해서는 dataset이 필요하다.
- 이 때 사용하는 dataset을 특성에 맞게 Train set, Validation set, Test set 3가지로 분류한다.

2. Train set
- 모델을 학습하기 위한 dataset이다.
- 어느 상황에서든 꼭 명심해야 하는 사실은 "모델을 학습하는데에는 오직 Train set만 이용한다" 이다.
- Train set을 이용해서 모델 구조에 변화를 주거나 epoch 등 다양한 hyperparameter를 바꿔가면서 다양한 실험을 진행한다.

3. Validation set
- 학습이 완료된 모델을 검증하기 위한 dataset이다.
- 헷갈리면 안되는 사실 중 하나가 validation set은 직접적으로 학습을 시키지는 않지만 '학습'에 관여는 한다.
- Train set처럼 data 자체를 학습시키지는 않지만 hyperparamter튜닝(어떤 parameter로 모델을 설계했을 때 가장 좋은 성능을 보이는지 테스트)과정에서 validation set이 사용된다.
- 딥러닝 모델의 목표는 결국 unseen data에 대해서 좋은 성능을 보이는 것이기에 validation set(unseen data)를 기반으로 가장 좋은 performance를 보이는 hyperparamter를 채택해 모델을 최종 설계한다.
- 즉, validation set이 중요한 이유는 train data에 overfitting되는 현상을 막아줌과 동시에 unseen data에 대해 좋은 성능을 보이는 모델을 설계하기 위함이다.

4. Test set
- 학습과 검증이 완료된 모델의 성능을 평가하기 위한 dataset이다.
- 학습에 사용되지도 않고, validation set과는 다르게 학습에 관여하지도 않는다.
- 순전히 unseen data로서 모델의 최종 성능을 평가하기 위해 사용된다.

#### Adam Optimizer
