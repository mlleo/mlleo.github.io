---
title: "[Google_Bootcamp_Day30]"
categories: 
  - ML
last_modified_at: 2020-12-01 12:00:00
comments: true
use_math: true # MathJax On
---
Algorithms for learning word embeddings

#### Neural language model

- Other contexts( ways to predict target words)
  - Last 4 words
  - 4 words on left & right
  - Last 1 word
  - Nearby 1word
  If you really want to build a language model, it's natural to use the last few words as a context. But if your main goal is really to learn a word embedding, then you can use all of these other contexts and they will result in very meaningful work embeddings as well
  


#### Word2vec algorithm

- Skip-grams model
  - the model uses the current(context) word to predict the surrounding window of context words
  - represents words as a vectors and learns to bring similar context words near to one another <br><br>
  
  ex. "The boy is going to school."
    - Assume "is" is current(context) word, and context size is 2
    - Input : "is", Output: "The", "boy", "going", "to"
  
![skip_gram](https://user-images.githubusercontent.com/62474292/101888880-12439a00-3be2-11eb-8b4e-b32b44135fb7.png)
  
  - Input layer<br> : input one-hot vector of current(context) word
  - Input -> Hidden<br> : get the value of multiplication(input * Embedding word matrix = embedding vector of current word)
  - Hidden -> Output<br> : get the value of multiplication(embedding vector * output word matrix) <br>
  (output word matrix : learnable parameter Theta_t)
  - Ouput<br> : apply softmax to get probability vector
  - Calculate loss between y_hat and y
  
  **Loss function**

  ![loss](https://user-images.githubusercontent.com/62474292/101888905-1b346b80-3be2-11eb-842b-0722559a7708.JPG)
  
#### Problem and Solutions of skip-gram model
- Computational speed 
  - Softmax unit : denominator should calculate the sum over the entire vocabulary size
  ![softmax](https://user-images.githubusercontent.com/62474292/101895054-5a66ba80-3bea-11eb-9eb0-dcb22e5e745f.png)
- Solution 1: Hierarchical softmax classifier (appropriate for rarely-used words)
- Solution 2: Negative Sampling (appropriate for frequently-used words)

#### How to sample the context c <br>
In practice the distribution of words isn't taken just entirely uniformly at random for the training set purpose, but instead there are different heuristics that you could use in order to balance out something from the common words together with the less common words.






#### Sentiment Classification

![senti](https://user-images.githubusercontent.com/62474292/102118811-8c467e00-3e83-11eb-8dfe-ded8c9de98ee.png)

- Problem : hard to have huge label training set
- Solution : word embeddings that you can take may help you to understand much better especially when you have a small training set <br>

#### Simple Sentiment Classification model
![sentiment](https://user-images.githubusercontent.com/62474292/102118436-09bdbe80-3e83-11eb-885a-7b860242c8a3.png)

- Problem : ignores word order
  - ex. "Completely lacking in good taste, good service, and good ambience" is actually negative review, but the word 'good' appears 3 times which means when we average it the ratings go up.

- Solution : Use many-to-one RNN model for sentiment classification
![rnn_sentiment](https://user-images.githubusercontent.com/62474292/102118447-0d514580-3e83-11eb-8394-860f367fd128.png)

  

[Source] https://www.coursera.org/learn/nlp-sequence-models
