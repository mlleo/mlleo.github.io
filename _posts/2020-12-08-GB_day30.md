---
title: "[Google_Bootcamp_Day30]"
categories: 
  - ML
last_modified_at: 2020-12-01 12:00:00
comments: true
use_math: true # MathJax On
---
Algorithms for learning word embeddings

#### Neural language model

- Other contexts( ways to predict target words)
  - Last 4 words
  - 4 words on left & right
  - Last 1 word
  - Nearby 1word
  If you really want to build a language model, it's natural to use the last few words as a context. But if your main goal is really to learn a word embedding, then you can use all of these other contexts and they will result in very meaningful work embeddings as well
  


#### Word2vec algorithm

- Skip-grams model
  - the model uses the current(context) word to predict the surrounding window of context words
  - represents words as a vectors and learns to bring similar context words near to one another <br><br>
  
  ex. "The boy is going to school."
    - Assume "is" is current(context) word, and context size is 2
    - Input : "is", Output: "The", "boy", "going", "to"
  
![skip_gram](https://user-images.githubusercontent.com/62474292/101888880-12439a00-3be2-11eb-8b4e-b32b44135fb7.png)
  
  - Input layer<br> :input one-hot vector of current(context) word
  - Input -> Hidden<br> : get the value of multiplication(input * Embedding word matrix = embedding vector of current word)
  - Hidden -> Output<br> : get the value of multiplication(embedding vector * output word matrix (learnable parameter) )
  - Ouput<br> : apply softmax to get probability vector
  - Calculate loss between y_hat and y
  
  **Loss function**

  ![loss](https://user-images.githubusercontent.com/62474292/101888905-1b346b80-3be2-11eb-842b-0722559a7708.JPG)
  
#### Problem and Solutions of skip-gram model
- Computational speed 
  - Softmax unit : denominator should calculate the sum over the entire vocabulary size
  
  
- Solution 1: Hierarchical softmax classifier (appropriate for rarely-used words)
- Solution 2: Negative Sampling (appropriate for frequently-used words)

  

[Source] https://www.coursera.org/learn/nlp-sequence-models
