---
title: "[Google_Bootcamp_Day16]"
categories: 
  - ML
last_modified_at: 2020-11-11 12:00:00
comments: true
use_math: true # MathJax On
---
ML strategy 2

#### Human-level performance
Some of the tasks that humans do are close to "perfection" which is why machine learning tries to mimic human-level performance. The graph below shows the performance of humans and machine learning over time.

![human_level](https://user-images.githubusercontent.com/62474292/101132271-0e47d300-364a-11eb-9b63-02cb3c538722.png)

Machine learning progresses slowly when it surpasses human-level performance. <br>
One of the reason is that **human-level performance can be close to Bayes optimal error**, especially for natural perception problem. Bayes optimal error is defined as the best possible error. In other words, it means that any functions mapping from x to y can't surpass a certain level of accuracy. <br>
Other reason is that when the performance of machine learning is worse than the performance of humans, you can improve it with different tools. **They are harder to use once it surpasses human-level performance.** <br><br>

These tools are ...
- get labeld data from humans
- gain insight rom manual error analysis
- better analysis of bias/variance

#### Avoidable bias
![bias_variance](https://user-images.githubusercontent.com/62474292/101159783-46aed780-3671-11eb-8ed9-748b63598ee9.png)

**Example: Cat vs Non-Cat**
![example](https://user-images.githubusercontent.com/62474292/101159789-47e00480-3671-11eb-8bbb-b7f96ed53144.png)

- Scenario A <br>
There is a 7% gap between the performance of the training set and the human level error. It means that the algorithm isn't fitting well with the training set since the target is around 1%. To resolve the issue, we use **bias reduction technique such as training a bigget neural network or running the training set longer.**

- Scenario B <br>
The training set is doing good since there is only a 0.5% difference with the human level error. The difference between the training set and the human level error is called avoidable bias. The focus here is to reduce the variance since the difference between the training error and the development error is 2%. To resolve the issue, we use **variance reduction technique such as regularization or have a bigger training set.**

#### Understanding human-level performance
Human-level error gives an estimate of Bayes error.

Example 1: Medical image classification <br>
This is an 


[Source] https://www.coursera.org/learn/machine-learning-projects
