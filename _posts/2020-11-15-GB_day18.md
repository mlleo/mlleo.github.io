---
title: "[Google_Bootcamp_Day18]"
categories: 
  - ML
last_modified_at: 2020-11-15 12:00:00
comments: true
use_math: true # MathJax On
---
ML strategy 4

#### Transfer Learning
Transfer learning refers to using the neural network knowledge for another application. <br><br>

Example: Cat recognition - radiology diagnosis 
![ex](https://user-images.githubusercontent.com/62474292/101225836-e7cf7980-36d5-11eb-9354-bd7b2b336312.png)

The following neural network is trained for cat recognition, but we want to adapt it for radiology diagnosis. The neural network will learn about the structure and the nature of images. This initial phase of training on image recognition is called **pre-training**, since it will pre-initialize the weights of the neural network. Updating all the weights afterwards is called **fine-tuning**.

#### Guideline
- Delete last layer of neural network
- Delete weights feeding into the last output layer of the neural network
- Create a new set of randomly initialized weights for the last layer only (if you have large dataset, then you can use more layers to re-train)
- Re-train on new dataset (x,y)

#### When to use transfer learning (Task A -> Task B)
- Task A and B have the same input x
- A lot more data for Task A than Task B
- Low level features from Task A could be helpful for Task B

#### Multi-Task Learning
Multi-task learning refers to having one neural network do simultaneously several tasks. <br><br>

Example: Simplified autonomous vehicle




[Source] https://www.coursera.org/learn/machine-learning-projects
