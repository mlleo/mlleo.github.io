---
title: "[Google_Bootcamp_Day18]"
categories: 
  - ML
last_modified_at: 2020-11-15 12:00:00
comments: true
use_math: true # MathJax On
---
ML strategy 4

#### Transfer Learning
Transfer learning refers to using the neural network knowledge for another application. <br><br>

Example: Cat recognition - radiology diagnosis 
![ex](https://user-images.githubusercontent.com/62474292/101225836-e7cf7980-36d5-11eb-9354-bd7b2b336312.png)

The following neural network is trained for cat recognition, but we want to adapt it for radiology diagnosis. The neural network will learn about the structure and the nature of images. This initial phase of training on image recognition is called **pre-training**, since it will pre-initialize the weights of the neural network. Updating all the weights afterwards is called **fine-tuning**.

#### Guideline
- Delete last layer of neural network
- Delete weights feeding into the last output layer of the neural network
- Create a new set of randomly initialized weights for the last layer only (if you have large dataset, then you can use more layers to re-train)
- Re-train on new dataset (x,y)

#### When to use transfer learning (Task A -> Task B)
- Task A and B have the same input x
- A lot more data for Task A than Task B
- Low level features from Task A could be helpful for Task B

#### Multi-Task Learning
Multi-task learning refers to having one neural network do simultaneously several tasks. <br><br>

Example: Simplified autonomous vehicle

![ex1](https://user-images.githubusercontent.com/62474292/101233402-15c7b480-36fc-11eb-8326-9c0d6238f0f9.png)
![multi_task](https://user-images.githubusercontent.com/62474292/101233399-13fdf100-36fc-11eb-9545-5058134d4807.png)

The vehicle has to detect simultaneously several things: pedestrians, cars, road signs, traffic lights, etc. We could have trained four separate neural networks, instead of train one to do four tasks. However, in this case, the performance of the system is better when one neural network is trained to do four tasks than training four separate neural networks **since some of the earlier features in the neural network could be shared between the different types of objects.**

#### When to use multi-task learning
- Training on a set of tasks that could benefit from having shared lower-level features
- Usually : Amount of data you have for each task is quite similar
- Can train a big enough neural network to do well on all tasks

#### End-to-End deep learning

End-to-End deep learning is the simplification of a processing or learning systems into one neural network. <br><br>

Example: Speech recognition model

![end_to_End](https://user-images.githubusercontent.com/62474292/101233401-152f1e00-36fc-11eb-9d4c-fb9c9af7451b.png)

End-to-End deep learning cannot be used for every problem since **it needs a lot of labeled data**. It is used mainly in audio transcripts, image captures, image synthesis, machine translation, steering in self-driving cars, etc.

#### Whether to use end-to-end learning

- Pros
  - Let the data speak
  - Less hand-designing of components needed
- Cons
  - May need large amount of data
  - Excludes potentially useful hand-designed components
  
- Key Question before applying end-to-end learning
  - Do you have sufficient data to learn a function of the complexity needed to map X to Y? <br> <br>


[Source] https://www.coursera.org/learn/machine-learning-projects
