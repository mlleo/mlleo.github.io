---
title: "[Google_Bootcamp_Day1]"
categories: 
  - ML
last_modified_at: 2020-11-03 12:00:00
comments: true
use_math: true # MathJax On
---
#### Google Machine Learning Bootcamp (Coursera Deep Learning Specialization)

GB(Google Bootcamp) series is based on the coursera lecture **"Deep Learning Specialization".**
These posts are for my review of basic machine learning, but I also hope that it could be helpful for others.<br><br>

- If you've done coursera lecture "Deep Learning Specialization", then it would be perfect for you to remind. <br>
- If you studied basic machine learning before, it could be nice to wrap up your knowledge. <br>
- If you haven't studied basic machine learning before, then I recommend you to use this blog posts as supplementary materials with **other full-organized materials(books, lectures, etc.)**

#### Standard notations for Deep Learning 
![notation](https://user-images.githubusercontent.com/62474292/102651645-430e6b00-41b0-11eb-8e1f-83a0c334ee82.png)

#### Binary Classification
- The result is a discrete value output
- Goal is to learn a classifier that can input an image represented by feature vector x, and predict whether the corresponding label y is 1 or 0, that is, whether this is a cat image or a non-cat image
![image](https://user-images.githubusercontent.com/62474292/102651642-4144a780-41b0-11eb-91b7-0cc95a602c3b.png)
- An image is stored in the computer in three separate matrices corresponding to Red, Green and Blue color channels of the image
- To create a feature vecotr x, the pixel value will be "reshape" for each color
- Dimension of the input feature vector x is n_x = 64 * 64 * 3 = 12,288
![feature_vector](https://user-images.githubusercontent.com/62474292/102651648-443f9800-41b0-11eb-9e23-2f57acc02640.png)

#### Logistic Regression
- A learning algorithm used in a supervised learning problem when the output y are either 0 or 1
- Goal of logistic regression is to minimize the error between its predictions and training data
![logistic](https://user-images.githubusercontent.com/62474292/102656125-bc10c100-41b6-11eb-8329-21bc1d61f150.png)
- wTx + b is a linear function, but since we are looking for a probability constraint between 0 and 1, so the sigmoid function is used
- If z is a large positive number, then sigma(z) = 1
- If z is small or large negative number, then sigma(z) = 0
- If z = 0, then sigma(z) = 0.5

#### Logistic Regression Cost function

1. Loss function
- measures the discrepancy between the prediction(y_hat) and the desired output(y)
- computes the error for a single training example

![loss1](https://user-images.githubusercontent.com/62474292/102680107-cd7abd00-41f8-11eb-8fd0-0b8df4c61035.png)
- This loss function has an optimization problem (multiple local optima), so gradient descent may not find global optimum
- Usually use the loss function below

![loss2](https://user-images.githubusercontent.com/62474292/102680105-cc499000-41f8-11eb-82b2-ec7c49dac196.png)

2. Cost function
- the average of the loss function of the entire training set
- try to find the parameters w and b that minimize the overall cost function
![cost](https://user-images.githubusercontent.com/62474292/102680106-cce22680-41f8-11eb-8792-d013c8ee1273.png)

![supp](https://user-images.githubusercontent.com/62474292/102680103-c94e9f80-41f8-11eb-82c6-53a14d5c2979.png)

#### Gradient Descent
![gradient](https://user-images.githubusercontent.com/62474292/102680590-f604b600-41fc-11eb-8059-e855d5c7df43.png)

#### Computation Graph
![computation](https://user-images.githubusercontent.com/62474292/102680591-f69d4c80-41fc-11eb-90b3-c89d60128879.png)

#### Computing derivatives
![derivatives](https://user-images.githubusercontent.com/62474292/102706831-a0054080-42d8-11eb-9373-7bfc0780b168.png)


[Source] https://www.coursera.org/learn/neural-networks-deep-learning

