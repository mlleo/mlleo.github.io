---
title: "[OS] Operating System Concepts CH 4-1"
categories: 
  - CS
last_modified_at: 2020-12-27 12:00:00
comments: true
use_math: true # MathJax On
---

#### Motivation for threads
- Problem
  - A single process cannot benefit from multi-cores
  - Very cucmberome to write a program with many cooperating processes
  - Expensive to create a new process
  - High communication overheads between processes
  - Expensive contect swithching between processes
- What if separate out a process's **execution state** from others
  - Process: address space, resources, other general process attributes(e.g. user ID, privileges, ...)
  - Execution state: PC, SP, registers, etc...
- Such an execution state is usually called **a thread**

#### Thread
- A thread executes a sequence of instructions in a program
- Single-threaded process
  - has one program counter specifying location of next instrution to execute
  - process executes instructions sequentially, one at a time, until completion
- Multi-threaded process
  - has one program counter per thread
- Threads **share an address space**
  - Code
  - Global variables
  - Heap
  - Opened files
- A thread **has its own**
  - Set of registers including PC & SP
  - Stack
  - Thread ID
![thread](https://user-images.githubusercontent.com/62474292/103384596-51f30700-4b3a-11eb-89f3-e54cc1156830.JPG)

#### Address space with Threads
![address](https://user-images.githubusercontent.com/62474292/103384671-97173900-4b3a-11eb-8bbd-5fea18f5821d.JPG)

#### Process vs Thread
- A process can have multiple threads
- A thread is bound to a single process
- Processes are containers in which threads execute
  -PID, address space, user and group ID, open file descriptors, current working directory, etc...
- All threads see the same address space
- Threads are the unit of scheduling

#### Multithreaded Server Architecture
![arch](https://user-images.githubusercontent.com/62474292/103385000-f0cc3300-4b3b-11eb-9c61-774f701f2ab2.JPG)

#### Benefits of Multi-threading
- Creating concurrency is cheap
- Allow to fully utilize multi-core architectures
- Resource sharing become obvious (Threads share the address space)
- Improves program structure (can divide large task across several cooperative threads)
- Throughput (by overlapping computation with I/O operations)
- Responsiveness (can handle concurrent events)

#### Concurrency vs Parallelism
![parallel](https://user-images.githubusercontent.com/62474292/103385495-0f332e00-4b3e-11eb-8b52-3246eda21651.JPG)

#### Types of Parallelism
- Data parallelism
  - Distributes subsets of the same data across multiple cores, **same operation on each**
- Task parallelism
  - Distributing threads across cores, each thread performing **unique operation**


#### Amdahl's Law
- Give performance gains in latency from adding additional system resources to an appliation
- When we add more cores to a process (S: serial portion, N: processing cores), then
![amdahl](https://user-images.githubusercontent.com/62474292/103390404-9e990b00-4b57-11eb-8955-7f1c2cca0b20.JPG)
- Serial portion of an application has dispropotionate effect on performance gained by adding additional cores
  - e.g. 25% serial, 75% parallel, increasing cores from 1 to 2 results in speedup of 1.625 times
  - As N approaches infinity, speedup approaches 1 / S

#### Concept Review
![concept](https://user-images.githubusercontent.com/62474292/103390405-9fca3800-4b57-11eb-8aba-e12a5f1b9a98.png)


