---
title: "[Google_Bootcamp_Day21]"
categories: 
  - ML
last_modified_at: 2020-11-19 12:00:00
comments: true
use_math: true # MathJax On
---

Convolutional Neural Networks2

#### Why convolutions?
- parameter sharing :
  A feature detector that's useful in one part of the image is probably useful in another part of the image
- sparsity of connections :
  In each layer, each output value depends only on a small number of inputs

#### Training process (same as standard nn)
![cnn_train](https://user-images.githubusercontent.com/62474292/100669622-03b6e080-33a1-11eb-9d91-196476fd8c84.png)

#### LeNet-5
![lenet-5](https://user-images.githubusercontent.com/62474292/100669621-031e4a00-33a1-11eb-9e9f-24496ae2c40d.png)

#### AlexNet
![alexnet](https://user-images.githubusercontent.com/62474292/100669623-044f7700-33a1-11eb-9ed4-8650ff1de636.png)

#### VGG-16
![vgg-16](https://user-images.githubusercontent.com/62474292/100669619-01548680-33a1-11eb-9548-5fc55ed10979.png)

#### ResNet
Problem of deep neural network : vanishing gradient problem <br>

**Residual Block**
![resnet_skip](https://user-images.githubusercontent.com/62474292/100709469-86678c00-33f1-11eb-9635-52930cbae777.png)

**Residual Network**
![resnet](https://user-images.githubusercontent.com/62474292/100709473-88c9e600-33f1-11eb-82d2-600d1e906a02.png)

#### Why Resnet works well?
