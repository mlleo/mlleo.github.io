---
title: "[ML BASIC REVIEW 3]"
categories: 
  - ML
last_modified_at: 2021-02-10 12:00:00
comments: true
use_math: true # MathJax On
---

#### ML_BASIC_REVIEW 3

#### Collaborative filtering

1. 배경지식
- Collaborative filtering에 대해 알기 전에 우선 추천시스템(Recommendation System)에 대해 알아야 한다. 
- 추천시스템이란 사용자의 취향을 파악해 다른 선택지를 제공해주는 시스템으로서 대표적으로 유투브, 넷플릭스 등 많은 서비스에서 사용되고 있다.
- 추천 시스템은 기본적으로 Content-based filtering 방식과 Collaborative filtering 방식이 존재한다.
- Content-based filtering 방식은 현재로서는 많이 사용되지는 않는다.(아래에 간략히 설명!)

2. Content-based filtering
- 사용자가 특정 아이템을 선호하는 경우 그 아이템과 비슷한 컨텐츠를 가진 다른 아이템을 추천해주는 방식이다.
- 예를 들어 사용자 A가 마블 관련 영화 X에 굉장히 높은 평점을 주었다면 다른 마블 영화를 추천해주는 것이다.
  ![CONTENT](https://user-images.githubusercontent.com/62474292/107618528-0c773d00-6c95-11eb-87a1-7da886817a87.JPG)

3. Collaborative filtering
- 많은 사용자로부터 얻은 취향 정보(user behavior)를 기반으로 하여 스스로 추천해주는 기술을 의미한다.
- Collaborative filtering은 nearest neighbor collaborative filtering (item-based, user-based)와 latent factor based collaborative filtering으로 나누어진다.

4. Nearest neighbor collaborative filtering
- User-Item matrix에서 사용자가 아직 평가하지 않은 아이템을 예측하는 것이 목표
- 아래 표는 기본적으로 DB가 저장하는 형태를 표현한 것! <br>
  ![표1](https://user-images.githubusercontent.com/62474292/107643779-f16af400-6cb9-11eb-9274-56174ec009ce.JPG) <br>
- User-based 방식에서는 데이터를 User(row) * Item(column)형태로 바꿔서 판단해야 함
- 아래 표의 예시를 보면 사용자 A와 B 모두 ItemA, ItemB, ItemC에 대해서 비슷한 평점을 주었으므로 사용자 A와 B는 유사한 사용자라고 판단(User-based) <br>
  ![표2](https://user-images.githubusercontent.com/62474292/107643781-f2038a80-6cb9-11eb-899b-520566430b80.JPG) 
- Item-based 방식에서는 데이터를 Item(row) * User(column)형태로 바꿔서 판단해야 함
- 아래 표의 예시를 보면 ItemA와 ItemB가 사용자들로부터 비슷한 평점을 받았으므로 ItemA와 ItemB를 비슷한 아이템이라고 판단(Item-based) <br>
  ![표3](https://user-images.githubusercontent.com/62474292/107644231-853cc000-6cba-11eb-80c9-8da9c76c1ea1.JPG) <br>
- 일반적으로 User-based collaborative filtering보다 Item-based collaborative filtering이 좀 더 정확도가 높다고 알려져 있다.

5. Latent factor based collaborative filtering
- 추천 시스템(Recommendation System) 방법 중 가장 많이 사용되는 알고리즘 중 하나로서 행렬 분해(matrix factorization)을 기반하여 사용한다.
- Matrix factorization이란 대규모 다차원 행렬을 SVD(Singular Value Decomposition)와 같은 차원 감소 기법으로 분해하는 과정에서 잠재 요인(latent factor)를 찾아내어 뽑아내는 과정이다.
- latent facotr를 찾아낸 이후 User-Item matrix를 아래와 같이 분해할 수 있다. <br>
  ![matrix](https://user-images.githubusercontent.com/62474292/107728991-e30cee80-6d32-11eb-816f-b142e25359a8.png) <br>
- 다음과 같이 분해된 두 행렬의 곱을 통해 사용자가 아직 평가하지 않은 컨텐츠의 점수를 예측할 수 있다.
- matrix factorization을 통해 latent factor를 추출함에 따라 저장 공간을 절약할 수 있다는 장점이 있다.


#### Few-shot Learning (+GPT-3 관련해서 추가 정리할 것!)

1. 배경지식
- 딥러닝 모델을 학습하는 데 있어서 가장 중요한 요소 중 하나가 데이터이다.
- 하지만 모델의 정확도를 높이기 위해 충분한 데이터를 확보하는 것은 많은 task에서 비현실적이고 달성하기 어렵다. (라벨링 비용)
- 이러한 제한된 데이터 및 시나리오에서 모델을 학습하기 위한 나온 학습 방법이 Few-shot learning이다.

2. Few-shot learning
- Few-shot learning이란 매우 적은 양의 train set으로 모델을 학습시키는 방법이다.
- 즉, network 학습시 class별로 적은 양의 이미지를 보여주고 network를 학습시키는 것이 Few-shot learning의 process이다.
- 아래 용어들은 Few-shot learning에서 주로 사용되는 용어들이다.
- n-way : 각 batch 별로 선택하는 label의 개수(N)
- K-shot : 각 class별로 선택하는 support data의 개수(K)
- support : 해당 batch에서 fine-tuning을 위해 학습하는 데이터셋 (train set)
- query : 해당 batch에서 class를 예측해야 하는 데이터셋 (test set)
- episode : 한 epoch당 수행하는 iteration 횟수 <br>
  ![meta](https://user-images.githubusercontent.com/62474292/107664101-698fe480-6ccf-11eb-8b8f-56f589b9c971.JPG) <br>
- Few-Shot Learning 에서 기존 Classification과의 가장 큰 차이점은 Sampling 수행 시 label의 index가 계속 바뀐다는 점이다 
- 위의 그림에서 알 수 있듯이 few-shot learning으로 학습한 모델이 잘 동작하기 위해서는 meta-learning이 필요한데 이 과정에서 다양한 task에 대해 episodic training을 진행하게 된다. 이러한 episodic training은 few-shot task와 유사한 형태의 train task를 통해 모델 스스로 학습 규칙을 도달해서 일반화 성능을 높일 수 있게 도와준다. 이 때, 각 task별로 support set이 sampling된 후 에 label의 index가 정해지기 때문에 기존 classification과는 다르다.


#### Federated Learning

1. 배경지식
- mobile device에 수많은 데이터가 축적되고 있지만 개인정보보호 문제로 server에 데이터 자체를 전송하는 것이 힘들다
- 서버에 데이터를 전송해서 학습하는 것이 아니라 각각의 device내에서 데이터를 직접 학습하고 나온 model paramter만을 server에 upload하자는 개념 (parameter는 개인정보가 아니므로!) 

2. Federated Learning Process
- 수집된 데이터를 통해 server에서 모델을 학습시킨다
- 학습된 모델을 여러 device에 deploy한다.
- 각각의 device에서 device내의 데이터를 기반을 학습이 이루어진다. (optimize model)
- 각각의 device별로 update된 paramter를 server에 upload한다. (gradient를 upload하는 경우도 있지만 weight를 device에서 계산 후 weight를 server에 올릴 수도 있다)
- 각각의 device별로 update된 paramter를 평균을 내어 server에 있는 모델을 update한 후 다시 여러 device에 deploy한다.

3. mobile device에서 server로 데이터를 전송하는 방법
- 대표적으로 FedSGD와 이를 발전시킨 FedAvg라는 알고리즘을 사용한다.
- 아래 그림은 일반적으로 FedSGD 알고리즘이 수행되는 과정을 보여준다 (왼쪽은 server에 gradient 전송, 오른쪽은 server에 weight 전송) <br>
  ![fed](https://user-images.githubusercontent.com/62474292/107717334-c2cf3680-6d16-11eb-96fb-ffdc998cef88.png)
- FedAvg 알고리즘에서는 각각의 device가 epoch가 batch_size를 별도로 가진 상태에서 mini-batch gradient descent를 수행하며, 업데이트된 paramter를 server에 upload한다.
- 예를 들어, epoch = 1이고 batch_size = full dataset이면 FedSGD와 동일하다.
- FedAvg 알고리즘은 상대적으로 FedSGD보다 device와 server의 communication이 덜 일어나도 되는 장점이 있다.

#### Central Limit Theorem

1. 정의
- 중심 극한 정리란 '동일한 확률분포를 가진 독립 확률 변수 n개의 표본평균 (X_bar)의 분포는 모집단의 분포에 상관없이 표본의 크기 n이 커질수록 정규분포에 가까워진다'라는 정리이다.
- 정확한 식을 표현하자면, '모집단이 **평균이 μ이고 표준편차가 σ인 임의의 분포**를 이룬다고 할 때, 이 모집단으로부터 추출된 **표본의 크기 n이 충분히 크다면** 표본 평균들이 이루는 분포는 **평균이 μ이고 표준편차가 √σ/μ인 정규분포**에 근접한다.
- 이 때 표본평균분포란 Sampling distribution of sample mean을 의미하는데, 이는 모집단에서 표본크기가 n인 표본을 여러번 반복해서 추출했을 때, 각각의 표본 평균들이 이루는 분포를 의미한다.
- 중심극한정리는 이 때 그 표본의 크기가 일정 수보다 크다면 (일반적으로 30보다 큰 경우를 가정) 표본평균의 분포를 정규분포로 가정할 수 있는 아주 유용한 정리이다.

2. 중요성
- 중심극한정리를 통해 모집단이 어떤 분포를 가지고 있던지 간에 (즉, 모집단 분포와 상관없이) 일단 표본의 크기가 충분히 크다면, 표본평균들의 분포가 모집단의 모수를 기반으로 한 정규분포를 이룬다는 점을 이용하여, 특정 사건(수집한 표본의 평균)이 일어날 확률값을 계산할 수 있다.
- 즉, 표본 평균들이 이루는 표본 분포와 모집단 간의 관계를 증명함으로써, 수집한 표본의 통계량을 이요하여 모집단의 모수를 추정할 수 있는 수학적 근거를 마련해준다.

#### Singular Value Decomposition

1. 배경지식
- 너무나도 많은 배경지식이 필요하지만 크게 eigen value, eigen vector, eigen value decomposition에 대해서만 정리!

2. Eigen Value, Eigen Vector
- 임의의 n * n 행렬 A에 대하여, AX=λX를 만족하는 0이 아닌 벡터 X가 존재한다면 λ는 행렬 A의 eigen value, 이에 대응하는 벡터 X를 eigen vector라고 한다.
- 위의 식을 바꾸면 (A-λI)X = 0이 되어 (A-λI)의 역행렬이 존재해야 되고 이는 det(A-λI) = 0으로 표현할 수 있다. 이를 통해 λ(eigen value)를 구할 수 있다.

3. Eigen Value Decomposition
- 임의의 n * n 행렬 A에 대하여 여러개의 eigen value와 eigen vector를 얻었다고 가정해보면 아래의 식을 만족하는 eigen value, eigen vector를 n개씩 얻을 수 있다. <br>
  ![1](https://user-images.githubusercontent.com/62474292/107785968-9b1cb480-6d90-11eb-8b4b-026084bb8f3c.JPG)
- 이러한 eigen vector를 모아둔 행렬 V(n * n)을 아래와 같이 생각하자. <br>
  ![2](https://user-images.githubusercontent.com/62474292/107785972-9bb54b00-6d90-11eb-8038-e31524d17dec.JPG)
- 행렬 A와 행렬 V의 관계를 위의 식을 이용해 한꺼번에 표현하면 다음과 같다. <br>
  ![3](https://user-images.githubusercontent.com/62474292/107785975-9c4de180-6d90-11eb-913f-6ca11162fc71.JPG)
- 이 때 eigen value를 대각성분에 모아둔 행렬 λ(n * n)를 다음과 같이 표시한다면 <br>
  ![4](https://user-images.githubusercontent.com/62474292/107785976-9c4de180-6d90-11eb-825d-e85e912db6ea.JPG)
- 다음과 같은 식이 도출될 수 있고 이는 행렬 A를 Eigen value를 통해 분해하는 과정을 보여준다. <br>
  ![5](https://user-images.githubusercontent.com/62474292/107785977-9ce67800-6d90-11eb-8547-3bc1e9c6a27f.png)

4. Singular Value Decomposition


