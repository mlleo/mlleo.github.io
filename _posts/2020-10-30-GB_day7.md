---
title: "[Google_Bootcamp_Day7]"
categories: 
  - ML
last_modified_at: 2020-10-30 12:00:00
comments: true
use_math: true # MathJax On
---

#### Normalizing inputs

![1](https://user-images.githubusercontent.com/62474292/102916551-1a93b300-44c7-11eb-869a-2bd4f3775e86.png)

- Use same mean, and variance to normalize test set

#### Why normalize inputs
![2](https://user-images.githubusercontent.com/62474292/102916559-1d8ea380-44c7-11eb-8891-57e4ad191e28.png)
- Easy and fast to optimize

#### Vanishing/Exploding gradients
![3](https://user-images.githubusercontent.com/62474292/102916562-1ebfd080-44c7-11eb-89f6-3523dd7c3227.png)
- Single neuron example
  - Partial solution for vanishing gradient problem is careful choice of random initialization
![4](https://user-images.githubusercontent.com/62474292/102916563-1ebfd080-44c7-11eb-83c1-e41cab3b0c73.png)

#### Numerical approximation of gradients
![5](https://user-images.githubusercontent.com/62474292/102916548-18315900-44c7-11eb-80fc-cff89454799d.png)

#### Gradient Checking
![6](https://user-images.githubusercontent.com/62474292/102916558-1cf60d00-44c7-11eb-8c92-e24b3539d5ee.png)
![7](https://user-images.githubusercontent.com/62474292/102916556-1c5d7680-44c7-11eb-8617-9d55ce9c1801.png)

- Don't use in training, only use to debug (slow computation)
- If algorithm fails grad check, look at components to try to identify bug
- Remember regularization
- Doesn't work with dropout (Turn off dropout, then grad check)
- Run at random initialization, perhaps again after some training


[Source] https://www.coursera.org/learn/deep-neural-network
