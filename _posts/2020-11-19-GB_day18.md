---
title: "[Google_Bootcamp_Day18]"
categories: 
  - ML
last_modified_at: 2020-11-19 12:00:00
comments: true
use_math: true # MathJax On
---

Convolutional Neural Networks 2

#### Why convolutions?
- parameter sharing : <br>
  A feature detector that's useful in one part of the image is probably useful in another part of the image
- sparsity of connections : <br>
  In each layer, each output value depends only on a small number of inputs

#### Training process (same as standard nn)
![cnn_train](https://user-images.githubusercontent.com/62474292/100669622-03b6e080-33a1-11eb-9d91-196476fd8c84.png)

#### LeNet-5
![lenet-5](https://user-images.githubusercontent.com/62474292/100669621-031e4a00-33a1-11eb-9e9f-24496ae2c40d.png)

#### AlexNet
![alexnet](https://user-images.githubusercontent.com/62474292/100669623-044f7700-33a1-11eb-9ed4-8650ff1de636.png)

#### VGG-16
![vgg-16](https://user-images.githubusercontent.com/62474292/100669619-01548680-33a1-11eb-9548-5fc55ed10979.png)

#### ResNet
Problem of deep neural network : vanishing gradient problem <br>

**Residual Block**
![resnet_skip](https://user-images.githubusercontent.com/62474292/100709469-86678c00-33f1-11eb-9635-52930cbae777.png)

**Residual Network**
![resnet](https://user-images.githubusercontent.com/62474292/100709473-88c9e600-33f1-11eb-82d2-600d1e906a02.png)

#### Why Resnet works well?
![why_resnet](https://user-images.githubusercontent.com/62474292/100712848-c54c1080-33f6-11eb-9f74-eda0dc53c942.png)

- z[l+2] , a[l] can be same dimmension by using "same" convolution
- easy to learn the identity function to just copy a[l] to a[l+2] using despite the addition of these two layers.


[source] https://www.coursera.org/learn/convolutional-neural-networks
