---
title: "[Google_Bootcamp_Day15]"
categories: 
  - ML
last_modified_at: 2020-11-09 12:00:00
comments: true
use_math: true # MathJax On
---

ML strategy 1

#### Why ML strategy important

- Assume we have cat classifer which have 90% performance, and want to improve the performance better.
- Then what should we do?
  - Collect more data
  - Collect more diverse training set
  - Train algorithm longer with gradient descent
  - Try Adam instead of gradient descent
  - Try dropout
  - Add L2 regularization
  - Change network architecture
  - etc...
- ML strategy is important to choose the most efficient way to improve your model

#### Orthogonalization

**Orthogonalization** or **orthogonality** is a system design property that assures that modifying an instruction or a component of an algorithm will not create or propagate side effects to other components of the system. It becomes easier to verify the algorithms independently from one another, and reduces testing and development time. <br> <br>

When a supervised learning system is design, these are the 4 assumptions that needs to be true and orthogonal.
- Fit training set well on cost function
  - If it doesn't fit well, try bigger network or Adam...
- Fit development set well on cost function
  - If it doesn't fit well, try regularization or bigger training set
- Fit test set well on cost function
  - If it doesn't fit well, try bigger dev set
- Performs well in real world
  - If it doesn't fit well, try to change dev set or cost function

#### Single number evaluation metric
![tp](https://user-images.githubusercontent.com/62474292/101052054-84582580-35c9-11eb-8d24-7be22cbaecb5.png)
![precision](https://user-images.githubusercontent.com/62474292/101052052-83bf8f00-35c9-11eb-8645-1b7c85c6f815.png)
![recall](https://user-images.githubusercontent.com/62474292/101052050-81f5cb80-35c9-11eb-9e33-7e7901b78508.png)
![f1score](https://user-images.githubusercontent.com/62474292/101052053-83bf8f00-35c9-11eb-9d75-1f0fec7fc51c.png)
![table](https://user-images.githubusercontent.com/62474292/101052057-84f0bc00-35c9-11eb-96d3-7922e773be7c.png)

- The problem with using precision/recall as the evaluation metric is that you are not sure which one is better
- F-1 score, a harmonic mean combine both precision and recall (Average also could be an evaluation metric)

#### Satisficing and optimizing metrics

There are different metrics to evaluate the performance of a classifier which are called evaluation metrics. They can be categorized as satisficing and optimizing metrics.
![metric](https://user-images.githubusercontent.com/62474292/101052858-75be3e00-35ca-11eb-80f9-d5c9c026e6ca.png)

- Assume we want to maximize accuracy, and running time <= 100ms
- In this example, accuracy is the optimizing metric and running time is the satisficing metric.
- If you have N metrics, then it should be 1 optimizing metric + (N-1) satisficing metric.
