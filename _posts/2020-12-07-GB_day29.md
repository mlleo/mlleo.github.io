---
title: "[Google_Bootcamp_Day29]"
categories: 
  - ML
last_modified_at: 2020-12-07 12:00:00
comments: true
use_math: true # MathJax On
---

#### Word representation
![onehot](https://user-images.githubusercontent.com/62474292/101322578-9cba8f80-38aa-11eb-8c21-e09c04fbdf27.png)

Problem of one-hot encoding
- it doesn't allow an algorithm to easily generalize words
- inner product between any two different one-hot vector is zero
- Example:
  - sentence 1: I want a glass of orange ________________ (blank : juice)
  - sentecne 2: I want a glass of apple __________________ 
  - With using one-hot encoding, there is no relation between "orange" and "apple" so that it is hard to learn the blank of sentence 2 is "juice".
  
#### Featurized representation
![feature](https://user-images.githubusercontent.com/62474292/101322575-9b896280-38aa-11eb-83a4-4bcc4b085ecf.png)

- ex. represent words as 300-dimensional vector
- representations for "orange" and "apple" are now quite similar
- allows it to generalize better across different words

#### Visualizaing word embeddings (ex. t-SNE)

300D -> 2D
![tsne](https://user-images.githubusercontent.com/62474292/101322579-9d532600-38aa-11eb-94eb-5737ffa3aa3d.png)

#### Using word embeddings

1. Learn word embeddings from large text corpus (1-100B words) or download pre-trained embedding online
2. Transfer embedding to new task with smaller training set (ex. 100K words)
3. Optional: Continue to fine-tune the word embeddings with new data (when data size of new dataset is big enough)

#### Relation to face encoding
![face](https://user-images.githubusercontent.com/62474292/101324378-819d4f00-38ad-11eb-8cef-a50b85fdc140.png)

- "Encoding" and "Embedding" has almost the same meaning
- In the case of "face encoding": train a neural network that can take as input any face picture even if it is new images, then compute an encoding for that new picture
- In the case of "word embedding": have a fixed vocabulary and just learns a fixed embedding for each of the words in our vocabulary

#### Analogy reasoning
![feature_vector](https://user-images.githubusercontent.com/62474292/101342520-098f5300-38c6-11eb-85f8-d709391ce529.png)
![plot](https://user-images.githubusercontent.com/62474292/101342524-0a27e980-38c6-11eb-8aa7-bcc88556d7c7.png)

- one of the remarkable results about word embeddings is the generality of analogy relationships they can learn

#### Cosine similarity
![cos](https://user-images.githubusercontent.com/62474292/101342519-098f5300-38c6-11eb-9d1b-7a250125cd6f.png)
![func](https://user-images.githubusercontent.com/62474292/101342513-085e2600-38c6-11eb-8d05-492557a0cfa5.png)

- if you learn a set of word embeddings and find a word w that maximizes this type of similarity, you can actually get the exact right answer

#### How to learn word embedding (Embedding matrix)
![embedding](https://user-images.githubusercontent.com/62474292/101344332-a9e67700-38c8-11eb-8886-ebeb070cd704.png)

- Assume the size of vocabulary is 10,000 (10K), and the word "orange" is 6257th
- Initialize E randomly and learn all the parameters of this 300 by 10,000 dimensional matrix, then E times this one-hot vector gives you the embedding vector
- In practice,  it is not efficient to actually implement this as a mass matrix vector multiplication because the one-hot vectors, now this is a relatively high dimensional vector and most of these elements are zero
- Use a specialized function to just look up a column of the Matrix E rather than do this with the matrix multiplication


[Source] https://www.coursera.org/learn/nlp-sequence-models
