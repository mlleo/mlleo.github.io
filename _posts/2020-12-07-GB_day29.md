---
title: "[Google_Bootcamp_Day29]"
categories: 
  - ML
last_modified_at: 2020-12-07 12:00:00
comments: true
use_math: true # MathJax On
---

#### Word representation
![onehot](https://user-images.githubusercontent.com/62474292/101322578-9cba8f80-38aa-11eb-8c21-e09c04fbdf27.png)

Problem of one-hot encoding
- it doesn't allow an algorithm to easily generalize words
- inner product between any two different one-hot vector is zero
- Example:
  - sentence 1: I want a glass of orange ______ (blank : juice)
  - sentecne 2: I want a glass of apple ______ 
  - With using one-hot encoding, there is no relation between "orange" and "apple" so that it is hard to learn the blank of sentence 2 is "juice".
  
#### Featurized representation
![feature](https://user-images.githubusercontent.com/62474292/101322575-9b896280-38aa-11eb-83a4-4bcc4b085ecf.png)

- representations for "orange" and "apple" are now quite similar
- allows it to generalize better across different words

#### Visualizaing word embeddings (ex. t-SNE)

300D -> 2D
![tsne](https://user-images.githubusercontent.com/62474292/101322579-9d532600-38aa-11eb-94eb-5737ffa3aa3d.png)

#### Using word embeddings

1. Learn word embeddings from large text corpus (1-100B words) or download pre-trained embedding online
2. Transfer embedding to new task with smaller training set (ex. 100K words)
3. Optional: Continue to fine-tune the word embeddings with new data (when data size of new dataset is big enough)

### Relation to face encoding


- "Encoding" and "Embedding" has almost the same meaning
- In the case of "face encoding": train a neural network that can take as input any face picture even if it is new images, then compute an encoding for that new picture
- In the case of "word embedding": have a fixed vocabulary and just learns a fixed embedding for each of the words in our vocabulary

[Source] https://www.coursera.org/learn/nlp-sequence-models
