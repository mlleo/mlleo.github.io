---
title: "[Google_Bootcamp_Day29]"
categories: 
  - ML
last_modified_at: 2020-12-07 12:00:00
comments: true
use_math: true # MathJax On
---

#### Word representation
![onehot](https://user-images.githubusercontent.com/62474292/101322578-9cba8f80-38aa-11eb-8c21-e09c04fbdf27.png)

Problem of one-hot encoding
- it doesn't allow an algorithm to easily generalize words
- inner product between any two different one-hot vector is zero
- Example:
  - sentence 1: I want a glass of orange ______ (blank : juice)
  - sentecne 2: I want a glass of apple ______ 
  - With using one-hot encoding, there is no relation between "orange" and "apple" so that it is hard to learn the blank of sentence 2 is "juice".
  
#### Featurized representation
![feature](https://user-images.githubusercontent.com/62474292/101322575-9b896280-38aa-11eb-83a4-4bcc4b085ecf.png)

- representations for "orange" and "apple" are now quite similar
- allows it to generalize better across different words

#### Visualizaing word embeddings (ex. t-SNE)

300D -> 2D
![tsne](https://user-images.githubusercontent.com/62474292/101322579-9d532600-38aa-11eb-94eb-5737ffa3aa3d.png)

[Source] https://www.coursera.org/learn/nlp-sequence-models
