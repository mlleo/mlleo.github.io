---
title: "[Google_Bootcamp_Day29]"
categories: 
  - ML
last_modified_at: 2020-12-07 12:00:00
comments: true
use_math: true # MathJax On
---

#### Word representation

Problem of one-hot encoding
- it doesn't allow an algorithm to easily generalize words
- inner product between any two different one-hot vector is zero
- Example:
  - sentence 1: I want a glass of orange ______ (blank : juice)
  - sentecne 2: I want a glass of apple ______ 
  - With using one-hot encoding, there is no relation between "orange" and "apple" so that it is hard to learn the blank of sentence 2 is "juice".
  
#### Featurized representation

- representations for "orange" and "apple" are now quite similar
- allows it to generalize better across different words

#### Visualizaing word embeddings (ex. t-SNE)

300D -> 2D


[Source] https://www.coursera.org/learn/nlp-sequence-models
