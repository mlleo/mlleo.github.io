---
title: "[ML BASIC REVIEW 2]"
categories: 
  - ML
last_modified_at: 2021-02-09 12:00:00
comments: true
use_math: true # MathJax On
---

#### ML_BASIC_REVIEW 2

#### CNN


#### Dropout

1. 배경시식
- 데이터를 통해 모델을 학습하다보면 train data에 대해 과도하게 학습이 되어 test data에 대한 오차가 더 증가하는 현상이 발생하는 데 이를 Train data에 Overfitting되었다고 한다. 
- 이는 Train data를 과도하게 학습하다보니 Train data에 대해서는 좋은 성능을 보여주고 Test data에 대해서는 좋지 못한 성능을 보여주게 되는 현상이다. (일반화 능력 떨어짐)
- 이를 해결하기 위한 방법 중 하나가 Dropout이다.

2. Dropout이란
- network의 hidden layer의 unit중 일부 unit만 동작하고 일부는 동작하지 않도록 하는 방법이다.
- 각 뉴런이 존재할 확률 p를 가중치 w와 곱해주는 형태가 되어 존재할 확률이 0인 경우 동작하지 않게 된다. (dropout시 parameter로 사용하지 않을 노드의 비율을 지정한다.)
- Dropout은 어느 하나의 feature에 의존하지 않기 위해 weight를 퍼트리겠다는 방식이다. 
- 주의할 점은 test과정에서는 dropout을 진행하지 않고 모든 node를 대상으로 test를 진행해야 한다. <br>
  ![KakaoTalk_20210212_221215652](https://user-images.githubusercontent.com/62474292/107772349-7bc95b80-6d7f-11eb-9771-493d36f6511c.png)

#### Batch Normalization


#### Instance Normalization


#### Layer Normalization


#### GAN


#### CycleGAN

