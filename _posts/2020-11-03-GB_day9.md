---
title: "[Google_Bootcamp_Day9]"
categories: 
  - ML
last_modified_at: 2020-11-03 12:00:00
comments: true
use_math: true # MathJax On
---

#### RMSprop (Root Mean Square prop)

![rms](https://user-images.githubusercontent.com/62474292/102927276-fee5d800-44d9-11eb-8af7-c8bb9bd898fc.png)

#### Adam optimization algorithm (RMSprop + momentum)
![adam](https://user-images.githubusercontent.com/62474292/102927266-fbeae780-44d9-11eb-8e72-e5e811c053e7.png)

- alpha : needs to be tune
- beta_1 : 0.9 (dw) -> recommended default value
- beta_2 : 0.999 (dw^2) -> recommended default value
- epsilon : 10^(-8) -> recommended default value

#### Learning rate decay
![rl](https://user-images.githubusercontent.com/62474292/102927277-fee5d800-44d9-11eb-9f24-b91bd8f321f1.png)
![rl_2](https://user-images.githubusercontent.com/62474292/102927271-fd1c1480-44d9-11eb-81aa-fb4d1dc1e987.png)

#### Other learning rate decay methods

#### Local optima in neural networks
![local](https://user-images.githubusercontent.com/62474292/102927275-fe4d4180-44d9-11eb-9a72-4038ed1baa90.png)

#### Problem of plateaus
- Plateaus : region where the derivative is close to zero for a long time
![plateaus](https://user-images.githubusercontent.com/62474292/102927273-fdb4ab00-44d9-11eb-8e45-6c49b1b15bab.png)


[Source] https://www.coursera.org/learn/deep-neural-network
