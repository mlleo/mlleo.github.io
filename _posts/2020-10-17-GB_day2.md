---
title: "[Google_Bootcamp_Day2]"
categories: 
  - ML
last_modified_at: 2020-10-17 12:00:00
comments: true
use_math: true # MathJax On
---

#### Logistic regression recap

![recap](https://user-images.githubusercontent.com/62474292/102706941-b4960880-42d9-11eb-9344-6f723b0fee7c.png)

#### Logistic regression derivatives
![logistic](https://user-images.githubusercontent.com/62474292/102706942-b52e9f00-42d9-11eb-9c2a-63bc05f103af.png)

#### 1 single step of Gradient descent on m examples
![implementation](https://user-images.githubusercontent.com/62474292/102707805-70a70180-42e1-11eb-83af-ebb823cdc638.png)

- 2 for loops (1 for m training examples and 1 or number of features) are inefficient
- Needs to be vectorized implementation

#### Vectorization
![vectorized](https://user-images.githubusercontent.com/62474292/102708028-4c4c2480-42e3-11eb-9865-db5941c5d83b.png)

- In neural network programming, whenever possible, avoid explicit for-loops

#### Logistic regression derivatives
![ex2](https://user-images.githubusercontent.com/62474292/102708030-4ce4bb00-42e3-11eb-9d8e-002516b4361c.png)

#### Vectorizing Logistic regression
- Before vectorization (with for loop)
![before](https://user-images.githubusercontent.com/62474292/102710959-9809c880-42f9-11eb-8ccb-2ae07854bae1.png)

- After vectorization (without for loop)
![after](https://user-images.githubusercontent.com/62474292/102710960-993af580-42f9-11eb-914d-a8aa74bd13f0.png)

#### Vectorizing Logistic regression's gradient computation
![gradient](https://user-images.githubusercontent.com/62474292/102711031-0babd580-42fa-11eb-88f4-3d1dc302e1d4.png)


[Source] https://www.coursera.org/learn/neural-networks-deep-learning
