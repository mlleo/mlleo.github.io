---
title: "[Google_Bootcamp_Day11]"
categories: 
  - ML
last_modified_at: 2020-11-07 12:00:00
comments: true
use_math: true # MathJax On
---

#### Batch Normalization
- makes hyperparameter search much easier

![1](https://user-images.githubusercontent.com/62474292/102929372-d5c74680-44dd-11eb-9f19-721af2a89379.png)

#### Normalizing inputs to speed up learning
- Normalizing input features help train 'w,b' more efficiently
- Same process to hidden layer
![2](https://user-images.githubusercontent.com/62474292/102929410-e1b30880-44dd-11eb-8ce7-196b4e71df2b.png)

#### Implementing Batch Norm
![3](https://user-images.githubusercontent.com/62474292/102929399-df50ae80-44dd-11eb-9333-a38bc5136409.png)

#### Adding Batch Norm to a network
![4](https://user-images.githubusercontent.com/62474292/102929404-e081db80-44dd-11eb-9d00-b4c527b3b42d.png)

#### With mini-batch
![5](https://user-images.githubusercontent.com/62474292/102929407-e11a7200-44dd-11eb-9066-5610f3d2eebd.png)

#### Implementing gradient descent
![6](https://user-images.githubusercontent.com/62474292/102929377-d6f87380-44dd-11eb-81d7-64a7c455a75b.png)

#### Why Batch norm work
- Problem
![7](https://user-images.githubusercontent.com/62474292/102929402-dfe94500-44dd-11eb-9b02-d52b08365494.png)
  - Even the exact value of Z1,Z2,...,Z4 (all in second layer) change, at least the mean and the variance remain same
- Solution
  - Batch Norm reduces the amount that the distribution of hidden unit values shift around
  - Limits the amount that updating parameters in the earlier layers can affect the distribution of values
  - Can use batch norm as regularization
    - Each mini-batch is scaled by the mean/variance computed on just that mini-batch
    - This adds some noise to the values z[l] within that minibatch.
    - So, similar to dropout, it adds some noise to each hidden layer's activations
    - large mini-batch size -> reduce noise -> reduce regularization effect

#### Batch Norm at test time
![1](https://user-images.githubusercontent.com/62474292/102929939-d90f0200-44de-11eb-9236-a992b139d625.png)

#### Softmax regression
- Ex. Recognizing cats(1), dogs(2), and baby chicks(3)
![2](https://user-images.githubusercontent.com/62474292/102929937-d8766b80-44de-11eb-9c26-e3672e011a6d.png)
- Softmax regression generalizes logistic regression to C classes


#### Softmax layer
![3](https://user-images.githubusercontent.com/62474292/102929938-d90f0200-44de-11eb-8b17-5bfd150365d0.png)
![4](https://user-images.githubusercontent.com/62474292/102929935-d7ddd500-44de-11eb-81e3-15ed5b85bd62.png)
![5](https://user-images.githubusercontent.com/62474292/102929932-d6aca800-44de-11eb-86b4-c16d3cab6bab.png)



[Source] https://www.coursera.org/learn/deep-neural-network
