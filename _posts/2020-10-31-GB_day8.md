---
title: "[Google_Bootcamp_Day8]"
categories: 
  - ML
last_modified_at: 2020-10-31 12:00:00
comments: true
use_math: true # MathJax On
---

#### Mini-batch gradient descent

- Vectorization allows you to efficiently compute on m examples
- But if m is too large, then it could be slow to train

![mini](https://user-images.githubusercontent.com/62474292/102925632-18d1eb80-44d7-11eb-9f74-1be6f1187353.png)
![code](https://user-images.githubusercontent.com/62474292/102925633-1a031880-44d7-11eb-9093-db07d889a393.png)

#### Training with mini-batch gradient descent
![graph](https://user-images.githubusercontent.com/62474292/102925636-1b344580-44d7-11eb-84bc-caf4dd500c90.png)

#### Choosing your mini-batch size
- If mini-batch size = m : Batch gradient descent
- If mini-batch size = 1 : Stochastic gradient descent (Every example is its own mini-batch)
- In practice, use somewhat between 1 and m <br>

1. Batch gradient descent
- Too long per iteration
2. Stochastic gradient descent
- Lose speed-up from vectorization
3. Mini-batch gradient descent
- Fastest learning
- Vectorization effect
- Make progress without processing entire training set <br>

If small training set (m <= 2000), then use gradient descent. <br>
Else use typical mini-batch size(64,128,256,512) <br>
Make sure that mini-batch fit in CPU/GPU memory

#### Exponentially weighted average
- Recent value : weight high, past value : weight low
- EX. Temperature in London

![ex1](https://user-images.githubusercontent.com/62474292/102926589-a2ce8400-44d8-11eb-820a-29f7a7ca7a1f.png)
![ex2](https://user-images.githubusercontent.com/62474292/102926580-9ea26680-44d8-11eb-9956-014712426d7f.png)
![ex3](https://user-images.githubusercontent.com/62474292/102926591-a3671a80-44d8-11eb-8632-6b1c6d928cd9.png)
![ex4](https://user-images.githubusercontent.com/62474292/102926574-9ba77600-44d8-11eb-8a1a-4bed1c1cfe7a.png)

#### Bias correction in exponentially weighted average
![bias](https://user-images.githubusercontent.com/62474292/102926258-0906d700-44d8-11eb-9ceb-6264febda7b0.png)

#### Gradient descent with momentum
- Gradient descent example
![momen](https://user-images.githubusercontent.com/62474292/102926248-06a47d00-44d8-11eb-9bba-fb3c2be3501e.png)
- Implementation of solution method
![imp](https://user-images.githubusercontent.com/62474292/102926271-0c01c780-44d8-11eb-8964-105a6a6db0ab.png)





[Source] https://www.coursera.org/learn/deep-neural-network
