---
title: "[Google_Bootcamp_Day4]"
categories: 
  - ML
last_modified_at: 2020-10-22 12:00:00
comments: true
use_math: true # MathJax On
---

#### Activation Functions

- Sigmoid
  - If z is too large/small, the gradient goes to zero which makes gradient descent slow down
  - Only exception to use sigmoid function is for the output layer in binary classification problem
- Tanh
  - Almost always better than sigmoid function
  - If z is too large/small, the gradient goes to zero which makes gradient descent slow down
- ReLU
  - Usually default choice of the activation function
  - If z < 0, the gradient goes zero, but enough hiddent units have z >= 0, so no worry for learning goes slow down
- Leaky ReLU

![activation](https://user-images.githubusercontent.com/62474292/102715573-bd0e3380-4319-11eb-8639-128e0ef46a22.png)

#### Why need non-linear activation functions
- Composition of linear activation functions is just a linear function
- It means that always same result no matter how many hidden units are

#### Derivatives of activation functions
- Sigmoid 
![sig](https://user-images.githubusercontent.com/62474292/102715692-871d7f00-431a-11eb-8db0-e041563fbdc5.png)

- Tanh
![tanh](https://user-images.githubusercontent.com/62474292/102715690-85ec5200-431a-11eb-82d5-553d7b51b0fe.png)

- ReLU
- Leaky ReLU
![relu](https://user-images.githubusercontent.com/62474292/102715693-871d7f00-431a-11eb-9379-02b176cebf6d.png)

#### Gradient descent for neural networks
![grad](https://user-images.githubusercontent.com/62474292/102716404-0dd45b00-431f-11eb-8d92-c3b6d46b823d.png)

#### Formulas for computing derivatives
![formula](https://user-images.githubusercontent.com/62474292/102716402-0ca32e00-431f-11eb-9f66-a8ef73d32f79.png)

#### Backpropagation review
- Logistic regression
![regression](https://user-images.githubusercontent.com/62474292/102717680-2d6f8180-4327-11eb-8078-54ad5c916c3a.png)

- Neural network
![nn](https://user-images.githubusercontent.com/62474292/102717684-2fd1db80-4327-11eb-9536-af571d7ac20c.png)

#### Summary of gradient descent
- Vectorize Implementation
![vectorized](https://user-images.githubusercontent.com/62474292/102717679-2c3e5480-4327-11eb-99da-57f392c580bc.png)
![summary](https://user-images.githubusercontent.com/62474292/102717683-2ea0ae80-4327-11eb-8509-27469c0d3e77.png)

#### Random Initialization

- For logistic regression, it was okay to initialize the weights to zero
- But for a neural network, if we initialize the weight parameters to all zero and then applied gradient descent, it won't work
- Initializing bias term to zero does not matter <br>
- Problem of initialization to zeros
  ![init_zero](https://user-images.githubusercontent.com/62474292/102717872-5a706400-4328-11eb-984a-f184a2045219.png)
- Random Initialization
  ![random_init](https://user-images.githubusercontent.com/62474292/102717873-5ba19100-4328-11eb-9a88-44b464ecfc38.png)
  

[Source] https://www.coursera.org/learn/neural-networks-deep-learning
