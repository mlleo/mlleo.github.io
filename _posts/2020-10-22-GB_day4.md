---
title: "[Google_Bootcamp_Day4]"
categories: 
  - ML
last_modified_at: 2020-10-22 12:00:00
comments: true
use_math: true # MathJax On
---

#### Activation Functions

- Sigmoid
  - If z is too large/small, the gradient goes to zero which makes gradient descent slow down
  - Only exception to use sigmoid function is for the output layer in binary classification problem
- Tanh
  - Almost always better than sigmoid function
  - If z is too large/small, the gradient goes to zero which makes gradient descent slow down
- ReLU
  - Usually default choice of the activation function
  - If z < 0, the gradient goes zero, but enough hiddent units have z >= 0, so no worry for learning goes slow down
- Leaky ReLU

![activation](https://user-images.githubusercontent.com/62474292/102715573-bd0e3380-4319-11eb-8639-128e0ef46a22.png)

#### Why need non-linear activation functions
- Composition of linear activation functions is just a linear function
- It means that always same result no matter how many hidden units are

#### Derivatives of activation functions
- Sigmoid 
![sig](https://user-images.githubusercontent.com/62474292/102715692-871d7f00-431a-11eb-8db0-e041563fbdc5.png)

- Tanh
![tanh](https://user-images.githubusercontent.com/62474292/102715690-85ec5200-431a-11eb-82d5-553d7b51b0fe.png)

- ReLU
- Leaky ReLU
![relu](https://user-images.githubusercontent.com/62474292/102715693-871d7f00-431a-11eb-9379-02b176cebf6d.png)

#### Gradient descent for neural networks



[Source] https://www.coursera.org/learn/neural-networks-deep-learning
