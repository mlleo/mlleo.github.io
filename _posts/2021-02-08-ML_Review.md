---
title: "[ML BASIC REVIEW]"
categories: 
  - ML
last_modified_at: 2021-01-01 12:00:00
comments: true
use_math: true # MathJax On
---

#### Grdient Descent

1. 직관적인 이해
- 길을 모르더라도 산 정상으로 가는 방법은 간단
- 현재 위치에서 가장 경사가 가파른 방향으로 산을 오르다 보면 언제가는 산 정상에 다다른다.
- 반대로 깊은 골짜기를 찾고 싶은 경우에는 가장 가파른 내리막 방향으로 산을 내려가다 보면 언제가는 산 골짜기에 다다른다. 
- 즉, 이와 같이 특정 함수의 극대점을 찾기 위해 현재 위치에서 gradient 방향으로 이동해가는 방법을 gradient ascent 방법, 극소점을 찾기 위해 gradient 반대 방향으로 이동해가는 방법을 gradient descent 방법이라 부른다. 

2. Gradient란
- 어떤 다변수 함수 f(x1,x2,...,xn)이 있을 때, 함수 f의 gradient는 아래와 같이 정의된다.

![gradient](https://user-images.githubusercontent.com/62474292/107191700-e866f200-6a2f-11eb-9e12-c4267701a816.JPG)

- 즉, gradient는 각 변수로의 일차 편미분 값으로 구성되는 벡터를 의미한다.
- 이 gradient 벡터는 함수 f의 값이 가장 가파르게 증가하는 방향을 나타낸다. (벡터의 크기는 그 증가의 가파른 정도, 기울기를 나타냄)
- 반대로, gradient 벡터에 음수를 취하게 되면 이는 함수 f값이 가장 가파르게 감소하는 방향을 나타낸다.
- 이러한 gradient 특성은 함수의 극점을 찾는 용도로 활용될 수 있다.

3. Grdient descent 방법
- 최적화 알고리즘 중 하나로서 gradient의 특성을 활용하여 특정 cost function의 값을 최소화시키기 위한 parameter값을 점진적으로 찾아내는 방법

![gradient_descent](https://user-images.githubusercontent.com/62474292/107192174-a1c5c780-6a30-11eb-9ab2-e07456c7826c.JPG)
- 즉, 어떤 초기값 벡터 x0부터 시작하여 위 식에 따라 gradient 반대 방향으로 x를 조금씩 이동시키면 함수 f(x)가 극소가 되는 x를 찾을 수 있다
- 이 때, lambda는 알고리즘의 수렴속도를 조절하는 paramter로 주로 learning rate이라 일컫어진다.
