---
title: "[Google_Bootcamp_Day28]"
categories: 
  - ML
last_modified_at: 2020-12-01 12:00:00
comments: true
use_math: true # MathJax On
---

#### Language model and sequence generation

P(y<1>, y<2>, y<3>, ..., y<Ty>) = ?
- Language model estimates the probability of that particular sequence of words
- Language model will be useful to represent sentences as outputs y rather than inputs x
- Training set : large corpus of text
  
#### Process of language modeling
1. Tokenize
2. Map each token to vocabulary set (one-hot encoding)
3. If not in vocabulary set, then change to UNK (unknown words)

#### RNN model

![rnn](https://user-images.githubusercontent.com/62474292/100941424-f20a4000-353c-11eb-9a60-75c591456a5f.png)
- Assume the example sentence is "Cats average 15 hours of sleep a day. <EOS>"
- a<1> make a softmax preiction to try to figure out what is the probability of the first words
- y<2> = P ( ___________________ | "cats")
- y<3> = P ( ___________________ | "cats average")
- P(y<1>, y<2>, y<3>) = P(y<1>) * P(y<2> | y<1>) * P(y<3> | y<1>, y<2>)
  
#### Loss function

![loss](https://user-images.githubusercontent.com/62474292/100941428-f33b6d00-353c-11eb-9552-6d37c2acff32.png)
