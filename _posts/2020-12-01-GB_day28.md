---
title: "[Google_Bootcamp_Day28]"
categories: 
  - ML
last_modified_at: 2020-12-01 12:00:00
comments: true
use_math: true # MathJax On
---

#### Language model and sequence generation

P(y<1>, y<2>, y<3>, ..., y<Ty>) = ?
- Language model estimates the probability of that particular sequence of words
- Language model will be useful to represent sentences as outputs y rather than inputs x
- Training set : large corpus of text
  
#### Process of language modeling
1. Tokenize
2. Map each token to vocabulary set (one-hot encoding)
3. If not in vocabulary set, then change to UNK (unknown words)

#### RNN model

![rnn](https://user-images.githubusercontent.com/62474292/100941424-f20a4000-353c-11eb-9a60-75c591456a5f.png)
- Assume the example sentence is "Cats average 15 hours of sleep a day. <EOS>"
- a<1> make a softmax preiction to try to figure out what is the probability of the first words
- y<2> = P ( ___________________ | "cats")
- y<3> = P ( ___________________ | "cats average")
- P(y<1>, y<2>, y<3>) = P(y<1>) * P(y<2> | y<1>) * P(y<3> | y<1>, y<2>)
  
#### Loss function

![loss](https://user-images.githubusercontent.com/62474292/100941428-f33b6d00-353c-11eb-9552-6d37c2acff32.png)

#### Sampling a sequence from a trained RNN language model
![sequence_model](https://user-images.githubusercontent.com/62474292/100943404-84601300-3540-11eb-9b3d-de34cea7885e.png)

- sample from Ttrained model's distribution to generate noble sequences of words.
- randomly sample according to this soft max distribution
- keep sampling until you generate an EOS token.
- Also could be character-level RNN as well as word-level RNN
  - pros : don't ever have to worry about unknown word tokens
  - cons : end up with much longer sequences so not as good as word level language models at capturing long range dependencies between how the the earlier parts of the sentence also affect the later part of the sentence
  
#### Vanishing gradients problem with RNNs
- baisc RNNs are not very good at capturing very long-term dependencies (similar to deep-layer NN)

#### Gated Recurrent Unit (GRU)

**RNN unit**
![rnn](https://user-images.githubusercontent.com/62474292/100971780-e63d6e80-357a-11eb-8fa6-7afe5e11c440.png)

**GRU unit**
![gru](https://user-images.githubusercontent.com/62474292/100971767-e473ab00-357a-11eb-83bc-7cf6f7af0a0e.png)
![gru2](https://user-images.githubusercontent.com/62474292/100971778-e5a4d800-357a-11eb-8031-46bceb113b46.png)

- element-wise multiplication tells your GRU which are the dimensions of your memory cell vector to update at every time-step, so you can choose to keep some bits constant while updating other bits.

**Full GRU**
![full_gru](https://user-images.githubusercontent.com/62474292/100971774-e5a4d800-357a-11eb-96b6-7982e9a3ecde.png)

- gate gamma r tells you how relevant is c<t-1> to computing the next candidate for c<t>
- have longer range connections, and also address vanishing gradient problems

#### Long short-term memory unit (LSTM)
![lstm_fig](https://user-images.githubusercontent.com/62474292/100975492-741c5800-3581-11eb-8a37-d51d5c3970e5.png)
![lstm_formula](https://user-images.githubusercontent.com/62474292/100975495-754d8500-3581-11eb-8db5-5d458f06dadc.png)

- slightly more powerful and more general version of the GRU
- separte forget gate and update gate

**Connect several LSTM units**
![lstm_connect](https://user-images.githubusercontent.com/62474292/100975493-74b4ee80-3581-11eb-8d4b-a5cf6cb7bbf6.png)

#### Bidirectional RNN

**Problem of unidirectional RNN**
- EX1. He said, "Teddy bears are on sale!"
- EX2. He said, "Teddy Roosevelt was a great President!"
- not enough to just look at the first part of the sentence to figure out whether the third word "Teddy" is a part of the person's name

**Solution = Bidirectional RNN**
![bidrectional_rnn](https://user-images.githubusercontent.com/62474292/100977838-e6db0280-3584-11eb-9fc3-ca1a2b3775c8.png)

- ex. in the case of "y_hat<3>", input x<1>, x<2>, x<3> comes from forward recurrent components(blue) and x<4> comes from backward recurrent components(red)
- bidirectional RNN with a LSTM units appears to be commonly used
- cons : need the entire sequence of data before you can make predictions anywhere

#### Deep RNNs
![deep_rnn](https://user-images.githubusercontent.com/62474292/100979100-b300dc80-3586-11eb-97e6-fe94dcaeb45e.png)

- deep RNNs are quite computationally expensive to train
- Because of the temporal dimension, these networks can already get quite big even if you have just a small handful of layers(3-layer is quite big)
- Another way is that you could have a bunch of deep layers after 3rd layer that are not connected horizontally

  

[Source] https://www.coursera.org/learn/nlp-sequence-models
