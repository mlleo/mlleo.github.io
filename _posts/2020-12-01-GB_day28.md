---
title: "[Google_Bootcamp_Day28]"
categories: 
  - ML
last_modified_at: 2020-12-01 12:00:00
comments: true
use_math: true # MathJax On
---

#### Language model and sequence generation

P(y<1>, y<2>, y<3>, ..., y<Ty>) = ?
- Language model estimates the probability of that particular sequence of words
- Language model will be useful to represent sentences as outputs y rather than inputs x
- Training set : large corpus of text
  
#### Process of language modeling
1. Tokenize
2. Map each token to vocabulary set (one-hot encoding)
3. If not in vocabulary set, then change to UNK (unknown words)
