---
title: "[Google_Bootcamp_Day28]"
categories: 
  - ML
last_modified_at: 2020-12-01 12:00:00
comments: true
use_math: true # MathJax On
---

#### Language model and sequence generation

P(y<1>, y<2>, y<3>, ..., y<Ty>) = ?
- Language model estimates the probability of that particular sequence of words
- Language model will be useful to represent sentences as outputs y rather than inputs x
- Training set : large corpus of text
  
#### Process of language modeling
1. Tokenize
2. Map each token to vocabulary set (one-hot encoding)
3. If not in vocabulary set, then change to UNK (unknown words)

#### RNN model

![rnn](https://user-images.githubusercontent.com/62474292/100941424-f20a4000-353c-11eb-9a60-75c591456a5f.png)
- Assume the example sentence is "Cats average 15 hours of sleep a day. <EOS>"
- a<1> make a softmax preiction to try to figure out what is the probability of the first words
- y<2> = P ( ___________________ | "cats")
- y<3> = P ( ___________________ | "cats average")
- P(y<1>, y<2>, y<3>) = P(y<1>) * P(y<2> | y<1>) * P(y<3> | y<1>, y<2>)
  
#### Loss function

![loss](https://user-images.githubusercontent.com/62474292/100941428-f33b6d00-353c-11eb-9552-6d37c2acff32.png)

#### Sampling a sequence from a trained RNN language model
![sequence_model](https://user-images.githubusercontent.com/62474292/100943404-84601300-3540-11eb-9b3d-de34cea7885e.png)

- sample from Ttrained model's distribution to generate noble sequences of words.
- randomly sample according to this soft max distribution
- keep sampling until you generate an EOS token.
- Also could be character-level RNN as well as word-level RNN
  - pros : don't ever have to worry about unknown word tokens
  - cons : end up with much longer sequences so not as good as word level language models at capturing long range dependencies between how the the earlier parts of the sentence also affect the later part of the sentence
  
#### Vanishing gradients problem with RNNs
- baisc RNNs are not very good at capturing very long-term dependencies (similar to deep-layer NN)

#### Gated Recurrent Unit (GRU)

**RNN unit**

**GRU unit**

- element-wise multiplication tells your GRU which are the dimensions of your memory cell vector to update at every time-step, so you can choose to keep some bits constant while updating other bits.

**Full GRU**

- gate gamma r tells you how relevant is c<t-1> to computing the next candidate for c<t>
